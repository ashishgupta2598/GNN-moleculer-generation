{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_size = 500\n",
    "hidden_node_features = 100\n",
    "\n",
    "class SummationMpnn(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_node_features = 500 #size from constant file\n",
    "        self.edge_features = 4\n",
    "        self.message_size = 100\n",
    "        self.message_passes = 3\n",
    "    def forward(self,nodes:tuple,edges:torch.tensor)->None:\n",
    "        adjacency = torch.sum(edges, dim=3)\n",
    "        print(\"adj is \",adjacency.shape)\n",
    "        (edge_batch_batch_idc,\n",
    "        edge_batch_node_idc,\n",
    "        edge_batch_nghb_idc) = adjacency.nonzero(as_tuple=True)\n",
    "        # print(\"edges various ares\",(edge_batch_batch_idc.shape,\n",
    "        # edge_batch_node_idc.shape,\n",
    "        # edge_batch_nghb_idc.shape))\n",
    "        #means picking out non zero waala indiced..gives connecting nodes\n",
    "        (node_batch_batch_idc,\n",
    "        node_batch_node_idc) = adjacency.sum(-1).nonzero(as_tuple=True)\n",
    "        print(\"nodes various ares\",(node_batch_batch_idc,node_batch_node_idc))\n",
    "\n",
    "        same_batch = node_batch_batch_idc.view(-1,1)== edge_batch_batch_idc\n",
    "        same_node  = node_batch_node_idc.view(-1, 1) == edge_batch_node_idc\n",
    "        print(\"same batch,node\",same_batch,   same_node)\n",
    "        message_summation_matrix = (same_batch * same_node).float()\n",
    "        print(\"message \",message_summation_matrix)\n",
    "        edge_batch_edges = edges[edge_batch_batch_idc, edge_batch_node_idc, edge_batch_nghb_idc, :]\n",
    "        print(\"edge_batch_edges \",edge_batch_edges.shape,'hello ',edges.shape)\n",
    "\n",
    "        hidden_nodes = torch.zeros(nodes.shape[0],\n",
    "                                   nodes.shape[1],\n",
    "                                   self.hidden_node_features,\n",
    "                                   device='cuda')\n",
    "        hidden_nodes[:nodes.shape[0], :nodes.shape[1], :nodes.shape[2]] = nodes.clone()#padding upto 13 nodes,with features to 100....we have only 9 features here\n",
    "        node_batch_nodes = hidden_nodes[node_batch_batch_idc, node_batch_node_idc, :]#picking out the same batch\n",
    "\n",
    "        print(\"yoyo \",node_batch_nodes.shape)\n",
    "        print(\"yoyo2\",hidden_nodes.shape)\n",
    "\n",
    "        for _ in range(self.message_passes):\n",
    "            edge_batch_nodes = hidden_nodes[edge_batch_batch_idc, edge_batch_node_idc, :]#getting hi  13*13*100\n",
    "\n",
    "            edge_batch_nghbs = hidden_nodes[edge_batch_batch_idc, edge_batch_nghb_idc, :]#getting neighs(hj)  13*13*1#why its 1\n",
    "            print(\"in message pass \",edge_batch_nghbs.shape, \"nodes \",edge_batch_nodes.shape)\n",
    "\n",
    "            message_terms    = self.message_terms(edge_batch_nodes,  \n",
    "                                                  edge_batch_nghbs,\n",
    "                                                  edge_batch_edges)\n",
    "\n",
    "            if len(message_terms.size()) == 1:  # if a single graph in batch\n",
    "                message_terms = message_terms.unsqueeze(0)\n",
    "\n",
    "            # the summation in eq. 1 of the NMPQC paper happens here\n",
    "            messages = torch.matmul(message_summation_matrix, message_terms)\n",
    "\n",
    "            node_batch_nodes = self.update(node_batch_nodes, messages)\n",
    "            hidden_nodes[node_batch_batch_idc, node_batch_node_idc, :] = node_batch_nodes.clone()\n",
    "\n",
    "        node_mask = adjacency.sum(-1) != 0\n",
    "        output    = self.readout(hidden_nodes, nodes, node_mask)\n",
    "        return hidden_nodes\n",
    "        #return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNN(SummationMpnn):\n",
    "    \"\"\"\n",
    "    The \"message neural network\" model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.constants       = constants\n",
    "        message_weights   = torch.Tensor(message_size,\n",
    "                                            hidden_node_features,\n",
    "                                            4)\n",
    "        if True:#== \"cuda\":\n",
    "            message_weights = message_weights.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        self.message_weights = torch.nn.Parameter(message_weights)\n",
    "\n",
    "        self.gru             = torch.nn.GRUCell(\n",
    "            input_size=message_size,\n",
    "            hidden_size = hidden_node_features,\n",
    "            bias=True\n",
    "        )\n",
    "        import math\n",
    "        #4 edge features\n",
    "        self.enn = MLP(\n",
    "            in_features=100,\n",
    "            hidden_layer_sizes=[250] * 1,\n",
    "            out_features=100,\n",
    "            dropout_p=0.2\n",
    "        )\n",
    "        msg_nns = MLP(4,[250]*4,4,dropout_p=0)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        stdev = 1.0 / math.sqrt(self.message_weights.size(1))\n",
    "        self.message_weights.data.uniform_(-stdev, stdev)\n",
    "\n",
    "    def message_terms(self, nodes,node_neighbours,edges):\n",
    "        edges_v = edges.view(-1,1,1)#it gives value for edges.....type of edge..size->(6,4)->in molecule with 6 edges kind\n",
    "        neighs = edges_v*node_neighbours.view(-1,1,1)#multiplying each by this number\n",
    "        outs = []\n",
    "        for i in range(1):\n",
    "            outs.append(nodes[:,i,:]*self.msg_nns[i](edges_v[:,i,:]))\n",
    "        output = sum(outs)\n",
    "        return output\n",
    "\n",
    "    def update(self, nodes, messages):\n",
    "        return self.gru(messages, nodes)\n",
    "    \n",
    "    def readout(self,nodes_out,nodes_in):#two different readout will be here.....1st and 2nf is adp readout\n",
    "        \n",
    "        #mlp is single linear layer....signle hidden neural net\n",
    "        \n",
    "        p = self.emb(torch.cat[nodes_out,nodes_in])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_features : int, hidden_layer_sizes : list, out_features : int,\n",
    "                 dropout_p : float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        activation_function = torch.nn.SELU\n",
    "        # create list of all layer feature sizes\n",
    "        fs = [in_features, *hidden_layer_sizes, out_features]\n",
    "        # create list of linear_blocks\n",
    "        layers = [self._linear_block(in_f, out_f,\n",
    "                                     activation_function,\n",
    "                                     dropout_p)\n",
    "                  for in_f, out_f in zip(fs, fs[1:])]\n",
    "        # concatenate modules in all sequentials in layers list\n",
    "        layers = [module for sq in layers for module in sq.children()]\n",
    "\n",
    "        # add modules to sequential container\n",
    "        self.seq = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def _linear_block(self, in_f : int, out_f : int, activation : torch.nn.Module,\n",
    "                      dropout_p : float) -> torch.nn.Sequential:\n",
    "        \n",
    "        # bias must be used in most MLPs in our models to learn from empty graphs\n",
    "        linear = torch.nn.Linear(in_f, out_f, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(linear.weight)\n",
    "        return torch.nn.Sequential(linear, activation(), torch.nn.AlphaDropout(dropout_p))\n",
    "\n",
    "    def forward(self, layers_input : torch.nn.Sequential) -> torch.nn.Sequential:\n",
    "        return self.seq(layers_input)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a7aeb117152383494077b830ed8f2bcff9728640e9e954d18ee6388f442a456"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
