{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpnn_train import network\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# hannas stark..spain germany...switerzland....dcn\n",
    "# graph net network in action..........matrix in 3d........brzil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "criterion = torch.nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<rdkit.Chem.rdmolfiles.SmilesMolSupplier object at 0x00000292269CF900>\n"
     ]
    }
   ],
   "source": [
    "from ashishcode import load_molecule,MolecularGraph,params\n",
    "path = \"data\\pre-training\\gdb13_1K\\Train.smi\"\n",
    "molecule_set = load_molecule(path)\n",
    "print(molecule_set)\n",
    "for mol in molecule_set:\n",
    "    molecule = MolecularGraph(mol)\n",
    "    # print(molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree is  1   [array([1, 3, 7], dtype=int64)]\n",
      "here is the value  [array([1, 3, 7], dtype=int64)]\n",
      "degree is  1   [array([ 8, 10], dtype=int64)]\n",
      "here is the value  [array([ 8, 10], dtype=int64)]\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "[] node index is as   12\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15408/1086297864.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmolecule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_decoding_route_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\graph gnn\\GraphINVENT\\ashishcode.py\u001b[0m in \u001b[0;36mget_decoding_route_state\u001b[1;34m(self, subgraph_idx)\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubgraph_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mmolecular_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncate_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mdecoding_APD\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmolecular_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_decoding_APD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[0mmolecular_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncate_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmolecular_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\graph gnn\\GraphINVENT\\ashishcode.py\u001b[0m in \u001b[0;36mget_decoding_APD\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim_f_conn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mlast_atom_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast_node_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_atoms\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mfv_nonzero_idc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nonzero_feature_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlast_node_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mf_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim_f_add\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\graph gnn\\GraphINVENT\\ashishcode.py\u001b[0m in \u001b[0;36mget_nonzero_feature_indices\u001b[1;34m(self, node_idx)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# feature vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"node index is as  \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[0msegment_idc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0midc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0msegment_idc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfv_idc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "molecule.get_decoding_route_state(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):#this we have to find......\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphinvent import gnn\n",
    "import torch\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        \"mlp1_depth\"          : 4,\n",
    "        \"mlp1_dropout_p\"      : 0.0,\n",
    "        \"mlp1_hidden_dim\"     : 100,\n",
    "        \"mlp2_depth\"          : 4,\n",
    "        \"mlp2_dropout_p\"      : 0.0,\n",
    "        \"mlp2_hidden_dim\"     : 100,\n",
    "        \"hidden_node_features\": 100,\n",
    "        \"message_passes\"      : 3,\n",
    "        \"message_size\"        : 100,\n",
    "        \"n_edge_features\"     :4\n",
    "    }\n",
    "import json\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "constants = dotdict(hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.n_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<rdkit.Chem.rdmolfiles.SmilesMolSupplier object at 0x000002A58F88D5E0>\n",
      "<ashishcode.MolecularGraph object at 0x000002A58F442E20>\n"
     ]
    }
   ],
   "source": [
    "from ashishcode import load_molecule,MolecularGraph,params\n",
    "path = \"data\\pre-training\\gdb13_1K\\Train.smi\"\n",
    "molecule_set = load_molecule(path)\n",
    "print(molecule_set)\n",
    "for mol in molecule_set:\n",
    "    molecule = MolecularGraph(mol)\n",
    "    print(molecule)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SummationMPNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract `SummationMPNN` class. Specific models using this class are\n",
    "    defined in `mpnn.py`; these are MNN, S2V, and GGNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, constants : namedtuple):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_node_features = constants.hidden_node_features\n",
    "        self.edge_features        = constants.n_edge_features\n",
    "        self.message_size         = constants.message_size\n",
    "        self.message_passes       = constants.message_passes\n",
    "        self.constants            = constants\n",
    "\n",
    "    def forward(self, nodes : torch.Tensor, edges : torch.Tensor) -> None:\n",
    "        adjacency = torch.sum(edges, dim=3)\n",
    "\n",
    "        # **note: \"idc\" == \"indices\", \"nghb{s}\" == \"neighbour(s)\"\n",
    "        (edge_batch_batch_idc,\n",
    "         edge_batch_node_idc,\n",
    "         edge_batch_nghb_idc) = adjacency.nonzero(as_tuple=True)\n",
    "        print(\"sizes are edge_batch_node_idc \",edge_batch_node_idc)\n",
    "\n",
    "        (node_batch_batch_idc, node_batch_node_idc) = adjacency.sum(-1).nonzero(as_tuple=True)\n",
    "\n",
    "        same_batch = node_batch_batch_idc.view(-1, 1) == edge_batch_batch_idc\n",
    "        same_node  = node_batch_node_idc.view(-1, 1) == edge_batch_node_idc\n",
    "\n",
    "        # element ij of `message_summation_matrix` is 1 if `edge_batch_edges[j]`\n",
    "        # is connected with `node_batch_nodes[i]`, else 0\n",
    "        message_summation_matrix = (same_batch * same_node).float()\n",
    "\n",
    "        edge_batch_edges = edges[edge_batch_batch_idc, edge_batch_node_idc, edge_batch_nghb_idc, :]\n",
    "\n",
    "        # pad up the hidden nodes\n",
    "        hidden_nodes = torch.zeros(nodes.shape[0],\n",
    "                                   nodes.shape[1],\n",
    "                                   self.hidden_node_features,\n",
    "                                   device=self.constants.device)\n",
    "        hidden_nodes[:nodes.shape[0], :nodes.shape[1], :nodes.shape[2]] = nodes.clone()\n",
    "        node_batch_nodes = hidden_nodes[node_batch_batch_idc, node_batch_node_idc, :]\n",
    "        \n",
    "\n",
    "        for _ in range(self.message_passes):\n",
    "            edge_batch_nodes = hidden_nodes[edge_batch_batch_idc, edge_batch_node_idc, :]\n",
    "\n",
    "            edge_batch_nghbs = hidden_nodes[edge_batch_batch_idc, edge_batch_nghb_idc, :]\n",
    "\n",
    "            print(\"hello ji \",edge_batch_nghbs.shape,edge_batch_nodes.shape,hidden_nodes.shape)\n",
    "\n",
    "            message_terms = self.message_terms(edge_batch_nodes,\n",
    "                                                  edge_batch_nghbs,\n",
    "                                                  edge_batch_edges)\n",
    "\n",
    "            if len(message_terms.size()) == 1:  # if a single graph in batch\n",
    "                message_terms = message_terms.unsqueeze(0)\n",
    "\n",
    "            # the summation in eq. 1 of the NMPQC paper happens here\n",
    "            messages = torch.matmul(message_summation_matrix, message_terms)\n",
    "\n",
    "            node_batch_nodes = self.update(node_batch_nodes, messages)\n",
    "            hidden_nodes[node_batch_batch_idc, node_batch_node_idc, :] = node_batch_nodes.clone() #updated the hidden states\n",
    "\n",
    "        node_mask = adjacency.sum(-1) != 0\n",
    "        output    = self.readout(hidden_nodes, nodes, node_mask)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pad(node,edge)->np.array:\n",
    "    n_size = node.shape\n",
    "    nodes_pad = np.zeros((13,9))\n",
    "    nodes_pad[:n_size[0],:n_size[1]] = node\n",
    "\n",
    "    #import numpy as np\n",
    "    e_size = edge.shape\n",
    "    edge_pad = np.zeros((13,13,4))\n",
    "    edge_pad[:e_size[0],:e_size[1],:] = edge\n",
    "\n",
    "    return nodes_pad,edge_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 4\n",
      "torch.Size([100, 100, 4])\n",
      "sizes are edge_batch_node_idc  tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "hello ji  torch.Size([24, 100]) torch.Size([24, 100]) torch.Size([1, 13, 100])\n",
      "hello ji  torch.Size([24, 100]) torch.Size([24, 100]) torch.Size([1, 13, 100])\n",
      "hello ji  torch.Size([24, 100]) torch.Size([24, 100]) torch.Size([1, 13, 100])\n",
      "input to network  torch.Size([1, 13, 100])\n",
      "api is  torch.Size([1, 1, 100])\n",
      "dims fadd ip torch.Size([1, 13, 100]) torch.Size([1, 1, 100]) torch.Size([1, 14, 100])\n",
      "global readout shapes  torch.Size([1, 1, 14, 100]) torch.Size([1, 1, 14, 100]) torch.Size([1, 1, 100])\n",
      "final shape  torch.Size([1, 29, 100])\n"
     ]
    }
   ],
   "source": [
    "network = MNN(constants)\n",
    "node,edge = molecule.get_graph_state()\n",
    "node,edge = pad(node,edge)#now we have padeed them according to max lenght\n",
    "nodes,edges = torch.Tensor(node).view((1,13,9)),torch.Tensor(edge).view((1,13,13,4))\n",
    "output = network(nodes,edges)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 4\n",
      "torch.Size([100, 100, 4])\n",
      "sizes are edge_batch_node_idc  tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8, 9, 9])\n",
      "hello ji  torch.Size([24, 100]) torch.Size([24, 100]) torch.Size([1, 13, 100])\n",
      "hello ji  torch.Size([24, 100]) torch.Size([24, 100]) torch.Size([1, 13, 100])\n",
      "hello ji  torch.Size([24, 100]) torch.Size([24, 100]) torch.Size([1, 13, 100])\n",
      "input to network  torch.Size([1, 13, 100])\n",
      "api is  torch.Size([1, 1, 100])\n",
      "dims fadd ip torch.Size([1, 13, 100]) torch.Size([1, 1, 100]) torch.Size([1, 14, 100])\n",
      "global readout shapes  torch.Size([1, 1, 14, 100]) torch.Size([1, 1, 14, 100]) torch.Size([1, 1, 100])\n",
      "final shape  torch.Size([986])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4609e+00,  6.6074e-01,  4.6993e-01,  2.9189e-01,  8.5182e-01,\n",
       "        -5.9710e-02, -2.6784e-02,  5.0952e-01,  5.3152e-01, -2.4074e-01,\n",
       "        -9.9794e-01,  7.3011e-01,  8.8378e-02, -5.9137e-01,  4.2911e-01,\n",
       "        -3.6567e-01, -8.8268e-01, -8.2736e-01,  5.0548e-02, -9.2682e-01,\n",
       "         1.8905e-01, -3.1485e-01,  8.8396e-01,  5.2260e-01, -8.4950e-01,\n",
       "        -7.3713e-01, -1.1603e+00,  1.5101e+00,  2.0267e-01, -1.5242e+00,\n",
       "        -9.2259e-01,  6.3304e-01,  1.5189e+00, -1.4491e+00, -9.2293e-01,\n",
       "        -1.4736e+00, -8.7386e-01, -2.2655e-01, -6.8803e-01, -1.0522e+00,\n",
       "        -1.4521e+00,  3.6297e-01, -1.0983e+00, -8.6399e-01, -2.2681e-01,\n",
       "        -4.6003e-01, -6.7433e-01, -2.4988e-01,  3.9334e-02,  1.5194e-01,\n",
       "         1.3868e+00,  2.8630e-01,  4.2581e-01, -1.2784e+00,  8.9422e-02,\n",
       "         7.8243e-01,  3.6766e-01,  3.2297e-01, -8.6034e-01,  6.3985e-01,\n",
       "         8.5803e-01, -3.2008e-01,  6.2596e-01,  4.5191e-02,  2.1000e-01,\n",
       "         1.6797e-02, -8.0690e-01, -1.4507e+00, -1.1544e+00, -9.2527e-01,\n",
       "        -1.7125e-01, -4.9657e-02,  5.6671e-01,  8.6912e-01, -3.8557e-01,\n",
       "        -9.5735e-01, -1.2978e+00,  7.3614e-01, -9.7057e-01,  4.3964e-01,\n",
       "        -4.2654e-01,  1.2765e+00, -1.4405e+00, -6.7462e-01,  1.4214e-01,\n",
       "         1.3798e+00,  2.2325e-01, -1.2291e+00, -1.2612e+00,  1.4333e+00,\n",
       "         9.7705e-01,  6.6616e-01, -1.0796e+00, -7.8519e-01, -2.7008e-01,\n",
       "        -7.6987e-01,  1.6834e+00,  1.3210e+00,  5.9177e-01, -3.5400e-01,\n",
       "        -1.1149e+00,  1.4510e+00,  7.1099e-01, -4.0196e-01, -1.4782e+00,\n",
       "        -1.2896e+00, -1.1555e+00,  6.8789e-02,  7.5493e-01, -1.3090e+00,\n",
       "        -1.4516e+00,  2.5477e-01, -1.9265e-01,  1.0793e+00,  8.9177e-01,\n",
       "         4.3589e-01,  2.0112e+00, -6.0532e-01,  4.5623e-01,  7.8375e-01,\n",
       "         5.5069e-01, -1.4677e-01, -1.0447e+00,  1.3944e+00, -4.2063e-01,\n",
       "        -4.2781e-01,  1.0185e+00,  1.2075e+00,  3.6378e-02,  9.7558e-01,\n",
       "        -7.7411e-01,  4.6280e-01, -1.0370e+00,  1.9948e+00,  6.3383e-01,\n",
       "        -7.2030e-02, -9.4122e-01,  1.3096e+00,  1.5900e-01,  6.6055e-01,\n",
       "         1.2592e+00,  3.7813e-01, -3.5852e-01,  1.0734e+00, -1.2368e+00,\n",
       "         1.5064e+00, -2.9641e-01, -7.2102e-03, -7.8581e-01,  6.9320e-01,\n",
       "         1.7206e-01, -1.4687e-01,  3.7074e-01, -4.3327e-02, -1.4431e+00,\n",
       "         8.4232e-01, -1.1579e+00,  1.4969e+00,  1.1701e+00, -9.3379e-01,\n",
       "        -1.2794e+00, -3.9600e-02, -1.1087e+00,  1.4278e+00,  5.7347e-01,\n",
       "        -2.6152e-02,  4.6303e-01, -3.8351e-01, -1.2173e+00,  2.2953e-01,\n",
       "         1.9329e-01, -9.5506e-01,  3.0675e-01,  1.7548e+00,  2.8412e-01,\n",
       "        -4.3379e-02,  1.0217e+00,  4.0715e-02, -1.0206e+00, -2.1203e-01,\n",
       "         1.1328e+00, -1.2231e+00,  3.6318e-01, -5.9429e-01, -1.2747e+00,\n",
       "         4.7792e-02,  3.0573e-01,  3.3193e-01,  5.3450e-01,  1.1552e+00,\n",
       "        -3.8850e-01,  2.6668e-01,  4.1291e-01,  5.9759e-01, -2.5189e-01,\n",
       "        -1.0696e+00, -1.5796e-01,  1.2416e+00,  3.4797e-01,  1.1743e+00,\n",
       "         4.3000e-01,  1.7122e+00,  8.6457e-01,  2.0986e+00,  5.5501e-01,\n",
       "         2.2252e-01, -3.1674e-01,  6.8315e-01, -1.1365e+00,  1.0528e-01,\n",
       "         1.6474e-01, -8.1395e-01, -9.6068e-01,  2.5127e-01, -6.4092e-01,\n",
       "         5.2129e-01,  1.4937e+00, -1.1020e+00, -8.8200e-01,  1.5511e+00,\n",
       "         3.3023e-01,  6.1814e-02,  1.0667e+00,  2.4186e-01, -4.0213e-02,\n",
       "         3.4002e-01,  1.3781e+00, -2.9019e-01,  1.2846e+00,  7.1259e-01,\n",
       "         6.2321e-01,  8.9655e-01,  1.9030e-01, -1.1701e+00,  1.1814e+00,\n",
       "         2.1474e-01, -2.9935e-01, -9.2576e-01,  2.9136e-01, -4.1077e-01,\n",
       "         5.4980e-01, -3.3488e-01,  1.9033e-01,  1.5997e+00,  1.0889e-01,\n",
       "         7.0121e-01,  1.2282e+00,  1.1721e-01,  2.3187e-01, -9.0175e-01,\n",
       "        -8.0064e-01,  4.1092e-01,  1.8981e-01,  1.1428e-01, -3.6836e-03,\n",
       "         5.6668e-02,  3.2913e-01,  2.5610e-01, -4.7877e-01,  1.4328e+00,\n",
       "        -1.0250e+00, -1.2230e+00, -8.9167e-01,  9.4445e-01,  9.5318e-01,\n",
       "        -9.8163e-01, -1.0389e+00,  6.6688e-02,  5.0371e-01,  1.2949e+00,\n",
       "         9.0570e-01,  5.4051e-02, -1.2491e+00,  3.8262e-01,  2.9628e+00,\n",
       "        -1.8575e-01, -1.2836e+00, -4.2507e-01,  1.3068e-01,  2.2187e-01,\n",
       "         6.2491e-01,  9.2741e-01, -8.2549e-01,  1.6680e+00,  2.1817e-01,\n",
       "        -5.9544e-01,  9.0117e-04,  3.3880e-01,  7.2635e-01,  2.3129e-01,\n",
       "         8.0556e-01,  3.1559e-01, -1.3847e+00,  3.5741e-01, -1.3928e+00,\n",
       "         7.7663e-01,  1.7702e+00, -5.8613e-01,  2.4303e-01, -5.7755e-01,\n",
       "        -1.1701e+00, -4.4440e-01, -6.4706e-02,  1.1807e+00, -1.4601e+00,\n",
       "         1.1698e+00,  3.9554e-01,  1.2496e+00, -2.0351e-01, -7.1462e-01,\n",
       "        -9.6246e-01, -1.0111e+00, -7.0792e-01, -1.3563e+00, -1.2407e+00,\n",
       "         1.0728e+00,  1.3519e+00, -6.5835e-01, -6.1315e-01,  6.4375e-01,\n",
       "        -1.0603e+00, -1.4140e-01,  1.1183e-01,  7.6499e-01, -9.2955e-02,\n",
       "         6.4331e-01, -4.6166e-01, -5.4881e-01,  1.2073e+00, -1.3953e+00,\n",
       "         5.5206e-01, -6.3538e-01,  2.1403e-01, -8.4031e-01, -2.6386e-01,\n",
       "        -7.2370e-01, -1.2817e+00, -5.8292e-01, -1.0666e+00,  1.1649e+00,\n",
       "         2.8740e-01, -1.1239e+00,  8.0551e-01,  8.0543e-01, -4.9244e-01,\n",
       "        -4.0783e-01,  2.2492e-01,  3.8658e-01,  3.9804e-01,  1.0426e-01,\n",
       "        -4.5669e-01, -6.3245e-01,  2.9153e-01,  7.3099e-01, -2.9579e-01,\n",
       "         7.3049e-02,  6.4821e-01, -1.3420e+00,  3.3775e-01, -1.5497e-01,\n",
       "        -4.8674e-01, -3.8767e-01, -1.0073e+00,  1.6325e+00, -1.2646e+00,\n",
       "        -1.1368e+00, -9.6371e-02, -1.2846e-01,  1.0150e+00, -5.2298e-01,\n",
       "         1.2727e+00, -6.5100e-02, -8.4207e-01,  2.1605e+00,  6.8071e-02,\n",
       "        -2.8323e-02,  1.0633e+00, -9.7226e-01,  5.1006e-01,  1.5739e-01,\n",
       "        -2.7554e-01,  2.4306e-02,  7.7431e-01,  6.1022e-01, -9.2718e-01,\n",
       "         2.8708e-01, -1.2807e+00,  1.7698e+00, -9.8931e-01, -1.2058e+00,\n",
       "        -5.7415e-01,  6.7661e-01, -1.4343e+00,  8.7296e-01, -4.3489e-01,\n",
       "         4.8207e-01, -8.2362e-01,  1.9092e+00,  4.8045e-01, -7.9632e-01,\n",
       "         4.8108e-01, -9.1850e-01, -8.0288e-01, -1.4923e+00, -1.2251e+00,\n",
       "         1.0184e+00,  9.5080e-01, -1.2574e+00,  4.4634e-01,  1.8559e-01,\n",
       "         2.1417e-01,  4.1868e-01,  2.0477e-01,  1.1605e+00, -1.4256e-01,\n",
       "         3.7197e-01, -3.4284e-01, -1.1243e+00, -3.2532e-01, -5.2189e-01,\n",
       "         5.4693e-01, -6.1998e-01, -2.9315e-01,  3.5056e-02, -1.1704e+00,\n",
       "         1.5608e+00, -1.0525e+00,  4.8821e-01,  1.2125e+00, -1.4989e+00,\n",
       "         1.0582e+00,  1.7880e-01, -2.1440e-01,  3.5642e-01, -3.2661e-01,\n",
       "        -1.1232e+00, -1.0187e+00,  6.6731e-01,  8.4786e-02, -1.1674e+00,\n",
       "         6.3265e-01,  1.7390e+00,  5.8115e-01,  1.0554e+00, -4.5926e-01,\n",
       "         1.0341e+00,  9.9674e-01, -5.1603e-01,  7.9734e-01,  1.0600e+00,\n",
       "        -1.3625e+00, -7.9104e-01, -5.1183e-01, -2.5118e-01, -6.5470e-01,\n",
       "         1.4778e-01,  1.5467e+00,  5.4253e-01,  1.6148e+00,  7.5033e-01,\n",
       "         7.9983e-01, -1.0130e+00,  5.7563e-01,  8.3156e-03, -1.1424e+00,\n",
       "        -1.2320e+00, -4.6418e-01, -2.0907e-01,  1.3355e-01, -1.2757e+00,\n",
       "         4.1058e-04,  1.4707e-02,  3.3139e-01,  2.1762e-01,  3.7340e-01,\n",
       "        -6.5600e-01, -1.3384e+00, -4.2805e-02,  8.0088e-01, -4.5056e-01,\n",
       "         1.5938e+00,  7.4034e-01, -1.4242e+00, -3.9565e-01,  1.3562e+00,\n",
       "        -2.8864e-01,  1.1985e+00,  8.0544e-01, -1.3423e+00,  9.7914e-01,\n",
       "         1.9726e-01,  9.1665e-03,  5.5311e-01,  5.3268e-01, -6.3371e-02,\n",
       "         2.8318e-01, -7.3560e-01, -8.3377e-01,  1.6213e+00,  5.3689e-03,\n",
       "         1.2482e+00,  2.5285e+00,  4.9539e-01,  9.5766e-01,  5.5548e-01,\n",
       "         4.0521e-01,  3.7545e-01, -9.1870e-01,  1.1721e+00, -2.0655e-03,\n",
       "         7.9884e-01,  2.8144e-01, -2.5377e-01, -8.7884e-01,  4.7636e-01,\n",
       "         4.6693e-01, -1.3244e+00, -3.3974e-01, -8.5244e-01,  2.6663e-01,\n",
       "         1.4917e+00,  7.2150e-01, -7.8707e-01,  1.4940e+00,  9.6738e-01,\n",
       "         1.8426e+00,  9.1457e-02, -1.6064e+00,  6.2946e-01,  1.6967e-01,\n",
       "         2.3485e-01,  2.0673e-02, -7.7264e-01,  4.5829e-02, -1.0331e-01,\n",
       "        -1.3969e+00, -1.4028e+00, -5.2774e-01, -2.6905e-01,  1.6430e-01,\n",
       "         2.9448e-01,  5.3768e-01,  7.2218e-01, -1.4044e+00,  3.0032e-01,\n",
       "         1.6488e+00, -1.2991e+00, -6.6106e-01,  1.7367e-01, -1.0621e+00,\n",
       "        -9.7844e-01, -8.0081e-01, -1.4423e+00, -1.5053e+00,  6.1930e-01,\n",
       "         3.1715e-01, -1.1933e+00,  4.0134e-01, -5.0178e-01, -3.7863e-01,\n",
       "        -9.8265e-01,  1.3306e-01, -7.1950e-01, -1.0427e+00, -1.1748e+00,\n",
       "        -1.3026e+00,  6.5042e-01, -1.0390e+00,  2.7921e-01,  4.2931e-01,\n",
       "        -2.8039e-01, -8.7705e-01, -8.0094e-02,  9.4325e-01,  1.8567e-01,\n",
       "         9.3519e-01, -2.2916e-01, -1.0215e+00, -1.2923e+00, -1.4853e+00,\n",
       "        -9.0113e-01,  1.2820e+00,  1.3556e+00,  4.5104e-01, -1.0090e+00,\n",
       "         1.6585e-02,  6.6149e-01, -3.9447e-01,  1.9443e+00,  8.4889e-01,\n",
       "        -8.7103e-01,  2.8255e+00, -1.1218e+00,  1.0685e+00,  9.5101e-01,\n",
       "         9.5012e-01,  2.2853e-01,  6.1041e-02,  6.4402e-01, -7.9814e-01,\n",
       "        -6.1676e-01, -1.1827e+00, -4.2783e-01,  8.8072e-01, -5.3140e-01,\n",
       "        -1.1237e+00, -1.8309e-01, -5.8765e-01,  1.3046e+00,  2.4476e-01,\n",
       "        -1.1364e+00,  7.3256e-01, -1.6940e-03,  9.9194e-02, -9.4696e-01,\n",
       "         1.7720e+00, -1.1751e+00,  6.1246e-01, -1.1389e+00, -6.3257e-01,\n",
       "        -3.9818e-01,  2.5933e-01, -1.3872e-01, -4.3037e-01,  1.8592e+00,\n",
       "         3.8433e-01, -1.2535e+00, -1.2607e+00,  4.1548e-01, -9.2497e-01,\n",
       "         1.2812e+00,  5.6359e-01, -8.4438e-01,  3.4877e-01,  1.1172e+00,\n",
       "        -1.5311e+00, -2.9651e-01, -1.2077e-01,  1.0198e+00,  4.4352e-01,\n",
       "        -1.1469e+00,  2.2347e-01, -6.7464e-01,  1.4027e+00,  8.3492e-01,\n",
       "         8.8017e-02, -8.1917e-01,  4.7563e-01,  4.7169e-01,  1.1982e+00,\n",
       "         1.4242e+00,  3.5465e-01,  2.6924e-01,  4.2082e-01,  1.1314e+00,\n",
       "         4.2880e-01,  5.5297e-01, -1.2219e+00, -1.5170e+00,  1.7494e+00,\n",
       "        -1.3490e+00,  2.8225e-01, -3.9927e-01,  1.4160e+00, -6.5099e-02,\n",
       "        -5.3829e-01, -1.2060e+00, -7.6466e-01, -1.4729e+00,  3.8304e-01,\n",
       "        -1.5952e-01,  2.3556e-01, -4.6182e-01,  1.0706e-01, -1.2077e+00,\n",
       "         9.8327e-01, -5.2007e-01,  7.9706e-01,  5.0130e-01, -6.1230e-01,\n",
       "         2.6206e-01,  1.5669e+00, -1.1064e+00,  4.2586e-01,  1.4240e+00,\n",
       "         5.0807e-04, -1.2839e+00,  1.1084e+00,  1.0632e-01, -5.0621e-01,\n",
       "        -2.1311e-01, -9.9909e-01, -4.5216e-01,  1.5547e+00,  3.5430e-01,\n",
       "         1.1496e+00,  4.2569e-01,  2.2743e-01,  6.8062e-01,  4.3397e-01,\n",
       "        -1.6403e-01,  2.0736e-01,  1.0744e+00,  9.9904e-01, -1.5664e+00,\n",
       "        -1.3457e+00, -9.6385e-01,  2.4714e+00,  5.2767e-01,  1.2819e+00,\n",
       "        -7.6676e-01,  2.2214e+00,  2.8855e-01,  4.1268e-01, -8.4570e-01,\n",
       "         1.5672e-01,  2.7717e-01,  1.7562e+00, -9.3345e-01, -1.0087e+00,\n",
       "         1.7101e-01, -8.8950e-01,  7.0616e-01,  8.5517e-01, -2.4806e-01,\n",
       "        -1.5660e-01, -1.0654e+00,  1.1987e+00, -6.6075e-01,  4.1971e-01,\n",
       "        -1.3896e+00, -8.4022e-01,  3.0199e-01, -5.0320e-01,  9.2313e-01,\n",
       "         9.7176e-01,  5.4357e-01,  1.2372e+00,  1.3689e+00,  3.9337e-01,\n",
       "        -1.3178e+00, -5.9484e-01,  1.1552e+00,  1.0584e+00,  1.4109e+00,\n",
       "        -8.4810e-01, -6.8841e-02, -1.2295e+00, -1.3322e+00,  5.0929e-01,\n",
       "        -1.9919e-01, -3.2154e-01, -9.8225e-01, -9.4763e-03,  8.9682e-01,\n",
       "         3.5069e-02, -9.8373e-01,  2.1536e-01, -9.4775e-01,  6.5710e-01,\n",
       "        -6.0623e-01, -7.3762e-01,  4.2962e-01,  7.9747e-01,  6.6061e-02,\n",
       "        -9.8911e-01,  1.6809e+00,  1.0578e+00,  1.1329e+00, -9.6991e-01,\n",
       "        -1.0252e+00, -5.1894e-01, -3.1609e-01, -8.7400e-01, -1.1425e+00,\n",
       "         9.1241e-01, -4.7138e-02,  4.3006e-01, -1.0196e+00, -4.2789e-01,\n",
       "         1.0129e+00,  2.1053e+00,  1.6034e-02, -8.9260e-01, -2.1746e-01,\n",
       "         9.3925e-02, -8.6197e-01, -8.8232e-01,  8.2234e-01,  1.1664e+00,\n",
       "        -2.3836e-01,  1.4138e+00, -3.3204e-01, -2.6635e-01, -6.8743e-02,\n",
       "        -2.8389e-02,  7.8203e-02, -1.2005e+00, -8.9942e-01, -1.2307e+00,\n",
       "         3.7385e-01,  2.6950e-01, -5.3467e-01, -5.3526e-01,  1.2179e+00,\n",
       "        -8.9466e-01,  3.7304e-01, -3.8128e-01, -5.0768e-01, -8.5678e-02,\n",
       "        -2.5052e-01, -9.2548e-01,  9.0243e-02, -3.0446e-01,  1.2834e+00,\n",
       "        -8.4524e-01, -1.1026e+00, -1.0343e+00,  2.8815e-01, -8.3123e-01,\n",
       "        -9.1870e-02,  4.5901e-01, -1.0935e+00,  2.6550e-01, -4.6076e-01,\n",
       "        -7.7961e-03, -1.2124e+00, -8.2609e-01,  2.2377e+00, -1.2177e-01,\n",
       "        -3.1916e-01, -5.2608e-01,  1.4214e+00,  6.7675e-01,  6.9711e-01,\n",
       "         1.1439e+00,  2.4035e-02,  5.0262e-01,  1.2557e-01, -1.1603e+00,\n",
       "        -4.9652e-01, -1.6843e-01, -6.9703e-01,  5.2195e-01,  2.2361e+00,\n",
       "        -1.3864e+00, -9.2148e-01,  1.0841e+00, -1.2086e+00, -1.3731e+00,\n",
       "         1.1147e-01, -8.4283e-02, -8.7462e-01, -6.2218e-01,  1.5901e-01,\n",
       "        -7.9652e-01,  9.1821e-01,  1.1298e+00, -9.7363e-01,  7.4011e-02,\n",
       "        -1.1002e+00,  8.8757e-01,  5.7739e-01, -1.2037e+00, -3.3547e-01,\n",
       "         2.4100e-01,  2.7572e-01,  7.1210e-01,  5.5255e-01,  3.3145e-01,\n",
       "         7.3251e-01, -6.5476e-02, -1.4234e+00,  3.9560e-01, -3.9003e-01,\n",
       "         1.1310e+00, -1.4540e+00, -2.6149e-01,  1.3892e-01, -1.2769e+00,\n",
       "        -5.0160e-01, -9.3962e-01, -9.2614e-01,  5.1358e-01, -9.0306e-01,\n",
       "        -1.0740e+00,  1.3671e+00,  5.5541e-01, -2.3409e-01,  4.5172e-01,\n",
       "        -1.0262e+00,  6.2697e-01, -1.4353e+00,  1.6373e-01,  3.0004e+00,\n",
       "         3.4912e-01, -7.0862e-01,  8.3096e-01, -5.8324e-01,  5.6277e-01,\n",
       "         7.4203e-01,  8.6932e-01, -1.3910e+00,  1.7930e-01, -8.1259e-01,\n",
       "        -6.0317e-01, -6.8460e-01,  6.0474e-01, -1.0534e+00,  1.3466e+00,\n",
       "         2.2191e-01,  1.6496e+00,  1.7125e-01, -6.7154e-01,  1.4734e+00,\n",
       "        -8.8055e-01, -6.9613e-01, -5.3362e-01, -8.0788e-01,  3.5578e-01,\n",
       "         1.2871e+00, -3.9875e-01,  1.1030e+00,  1.3202e+00, -8.6379e-01,\n",
       "        -3.7728e-01, -1.5193e+00, -5.7983e-01, -1.1557e+00, -1.9281e-01,\n",
       "        -9.9216e-01, -7.4779e-01,  4.9880e-02,  1.1820e+00, -5.3344e-01,\n",
       "        -4.4338e-01,  7.0416e-01,  7.7311e-01, -1.3469e+00, -1.0793e+00,\n",
       "         1.3387e+00, -9.2932e-01,  6.6817e-01, -1.1696e+00, -6.8621e-01,\n",
       "        -1.4157e+00,  2.8415e-02,  1.2986e+00, -1.1209e+00, -6.0883e-01,\n",
       "        -4.3008e-01, -1.0529e+00, -6.4507e-01,  2.3395e-01, -1.2941e+00,\n",
       "         7.5028e-01, -7.3841e-01, -5.3264e-01, -2.4969e-01, -1.0615e+00,\n",
       "        -6.8349e-01,  1.1518e+00, -8.6673e-01, -8.4227e-01,  6.5342e-01,\n",
       "        -5.5060e-02,  1.1402e+00,  1.1849e+00,  1.6791e+00,  5.4234e-01,\n",
       "         7.5097e-02,  1.2428e+00,  6.6955e-01,  1.5407e-01, -5.4019e-01,\n",
       "         3.5355e-01,  3.8765e-01,  3.9274e-01,  2.1857e+00, -8.7801e-01,\n",
       "        -5.6456e-02, -1.1202e-01, -1.5200e+00,  1.0740e+00, -2.2506e-01,\n",
       "         1.3777e-01], grad_fn=<EluBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = MNN(constants)\n",
    "node,edge = molecule.get_graph_state()\n",
    "# nodes,edges = torch.Tensor(node).view((1,13,9)),torch.Tensor(edge).view((1,13,13,4))\n",
    "network(nodes,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graphinvent.gnn.modules import GlobalReadout \n",
    "class MNN(SummationMPNN):\n",
    "    def __init__(self,constants) -> None:\n",
    "        super().__init__(constants)\n",
    "        self.constants       = constants\n",
    "        print(self.constants.message_size,self.constants.hidden_node_features,4)\n",
    "        message_weights      = torch.Tensor(self.constants.message_size,\n",
    "                                            self.constants.hidden_node_features,\n",
    "                                            4)#edge features\n",
    "        print(message_weights.shape)\n",
    "        if False:#\"cuda\" == \"cuda\":\n",
    "            message_weights = message_weights.to(\"cuda\", non_blocking=True)\n",
    "        \n",
    "    \n",
    "        self.message_weights = torch.nn.Parameter(message_weights)\n",
    "\n",
    "        self.gru             = torch.nn.GRUCell(\n",
    "            input_size=self.constants.message_size,\n",
    "            hidden_size=self.constants.hidden_node_features,\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        self.APDReadout = GlobalReadout()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        import math\n",
    "        stdev = 1.0 / math.sqrt(self.message_weights.size(1))\n",
    "        self.message_weights.data.uniform_(-stdev, stdev)\n",
    "\n",
    "    def message_terms(self, nodes : torch.Tensor, node_neighbours : torch.Tensor,\n",
    "                        edges : torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        edges_view            = edges.view(-1, 1, 1, self.constants.n_edge_features)\n",
    "        #print(\"edges \",edges_view.shape)\n",
    "        weights_for_each_edge = (edges_view * self.message_weights.unsqueeze(0)).sum(3)\n",
    "        return torch.matmul(weights_for_each_edge,\n",
    "                            node_neighbours.unsqueeze(-1)).squeeze()\n",
    "    #torch.broadcast_to(x, (3, 3))\n",
    "    def update(self, nodes : torch.Tensor, messages : torch.Tensor) -> torch.Tensor:\n",
    "        return self.gru(messages, nodes)\n",
    "\n",
    "    def readout(self, hidden_nodes : torch.Tensor, input_nodes : torch.Tensor,\n",
    "                node_mask : torch.Tensor) -> torch.Tensor:\n",
    "        # graph_embeddings = torch.sum(hidden_nodes, dim=1)\n",
    "        print(\"input to network \",hidden_nodes.shape)\n",
    "        output           = self.APDReadout(hidden_nodes)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'molecule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15408/2518384292.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmolecule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_decoding_APD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mash\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'molecule' is not defined"
     ]
    }
   ],
   "source": [
    "ash = molecule.get_decoding_APD()\n",
    "ash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "katai.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10, 4])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalReadout(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp1 = MLP(in_features=constants.message_size,\n",
    "                  hidden_layer_sizes=[constants.mlp1_hidden_dim]*constants.mlp1_depth,\n",
    "                  out_features=constants.message_size,\n",
    "                  dropout_p=0.0)\n",
    "        self.mlp2 = MLP(in_features=constants.message_size,\n",
    "                  hidden_layer_sizes=[constants.mlp2_hidden_dim]*constants.mlp2_depth,\n",
    "                  out_features=constants.message_size,\n",
    "                  dropout_p=0.0)\n",
    "        \n",
    "        self.mlp3 = MLP(in_features=constants.message_size,\n",
    "                  hidden_layer_sizes=[constants.mlp1_hidden_dim]*constants.mlp1_depth,\n",
    "                  out_features=constants.message_size,\n",
    "                  dropout_p=0.0)\n",
    "        self.mlp4 = MLP(in_features=2*constants.message_size,\n",
    "                  hidden_layer_sizes=[constants.mlp2_hidden_dim]*constants.mlp2_depth,\n",
    "                  out_features=constants.message_size,\n",
    "                  dropout_p=0.0)\n",
    "        self.mlpt = MLP(in_features=constants.message_size,\n",
    "                  hidden_layer_sizes=[constants.mlp1_hidden_dim]*constants.mlp1_depth,\n",
    "                  out_features=constants.message_size,\n",
    "                  dropout_p=0.0)\n",
    "        self.final_l = MLP(in_features=2900,#23*100 #(14+14+1)100\n",
    "                  hidden_layer_sizes=[500]*1,\n",
    "                  out_features=989,\n",
    "                  dropout_p=0.0)\n",
    "          \n",
    "    def forward(self,features):\n",
    "        g= torch.sum(features,dim=1)\n",
    "        g = g.view(1,1,100)\n",
    "        #g = torch.broadcast_to(g, (1,10, 100))\n",
    "        print(\"api is \",g.shape)\n",
    "        fadd1 = self.mlp1(features)\n",
    "        fconn1 = self.mlp2(features)  \n",
    "\n",
    "        print(\"dims fadd ip\",fconn1.shape,g.shape,torch.cat([fadd1,g],dim=1).shape)\n",
    "        fadd = self.mlp3(torch.cat([fadd1,g],dim=1)).unsqueeze(dim=1)\n",
    "        fconn = self.mlp3(torch.cat([fconn1,g],dim=1)).unsqueeze(dim=1)\n",
    "\n",
    "        fterm = self.mlpt(g)\n",
    "        print(\"global readout shapes \",fadd.shape,fconn.shape,fterm.shape)\n",
    "        cat = torch.cat((fadd.squeeze(dim=1), fconn.squeeze(dim=1), fterm), dim=1)\n",
    "        cat = torch.flatten(cat)\n",
    "        cat = self.final_l(cat)\n",
    "        print(\"final shape \",cat.shape)\n",
    "\n",
    "        return cat\n",
    "        #apd = self.Softmax()....from original code its removed\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_features : int, hidden_layer_sizes : list, out_features : int,\n",
    "                 dropout_p : float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        activation_function = torch.nn.SELU\n",
    "        # create list of all layer feature sizes\n",
    "        fs = [in_features, *hidden_layer_sizes, out_features]\n",
    "        # create list of linear_blocks\n",
    "        layers = [self._linear_block(in_f, out_f,\n",
    "                                     activation_function,\n",
    "                                     dropout_p)\n",
    "                  for in_f, out_f in zip(fs, fs[1:])]\n",
    "        # concatenate modules in all sequentials in layers list\n",
    "        layers = [module for sq in layers for module in sq.children()]\n",
    "\n",
    "        # add modules to sequential container\n",
    "        self.seq = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def _linear_block(self, in_f : int, out_f : int, activation : torch.nn.Module,\n",
    "                      dropout_p : float) -> torch.nn.Sequential:\n",
    "        \n",
    "        # bias must be used in most MLPs in our models to learn from empty graphs\n",
    "        linear = torch.nn.Linear(in_f, out_f, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(linear.weight)\n",
    "        return torch.nn.Sequential(linear, activation(), torch.nn.AlphaDropout(dropout_p))\n",
    "\n",
    "    def forward(self, layers_input : torch.nn.Sequential) -> torch.nn.Sequential:\n",
    "        return self.seq(layers_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(100,1000, bias=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = torch.zeros((10,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.shape\n",
    "edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(ip).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a7aeb117152383494077b830ed8f2bcff9728640e9e954d18ee6388f442a456"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
